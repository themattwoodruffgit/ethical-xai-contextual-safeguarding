# Outputs Directory

This directory contains example outputs from the three main analysis scripts. These outputs demonstrate the expected results and format when running the analyses with your own data.

## Directory Structure

```
outputs/
├── comparative/    # Results from Results-1-comparative.py
├── fairness/       # Results from Results-2-fairness-ebm.py
├── hybrid/         # Results from Results-3-hybrid.py
└── README.md       # This file
```

## Output Files by Script

### 1. Comparative Analysis (`comparative/`)

Generated by `scripts/Results-1-comparative.py`

#### Files:
- **`combined_results.csv`** - Performance metrics table
  - Rows: 6 algorithms × 3 models = 18 combinations
  - Columns: Accuracy, Precision, Recall, Specificity, F1-score, ROC-AUC

- **`model_performance_comparison.png`** - Visualisation (1200×800px)
  - Comprehensive performance comparison
  - Grouped by model type (Base, Full, CSRI)
  - Shows all metrics for all algorithms

- **`model_performance_comparison.pdf`** - Publication-quality version
  - High-resolution vector graphics
  - Suitable for thesis inclusion

#### Console Output:
```
PyGOL Rules for Base Model:
-----------------------------
Rule 1: safeguarding_any(X) :- fsm_everfsm6(X,1), currentncyear(X,Y), Y > 8.
...

PyGOL Rules for CSRI Model:
-----------------------------
Rule 1: safeguarding_any(X) :- peer_relationships(X,high_risk), fsm_everfsm6(X,1).
...
```

### 2. Fairness Testing (`fairness/`)

Generated by `scripts/Results-2-fairness-ebm.py`

#### Files:
- **`ebm_csri_global_importance.csv`** - Feature importance rankings
  - Feature names and importance scores
  - Sorted by importance (descending)

- **`ebm_csri_top_importances.png`** - Feature importance visualisation
  - Bar chart of top 15 features
  - Shows relative importance

- **`ebm_csri_top_case_reasons.txt`** - Local explanations (if available)
  - Detailed explanations for high-risk cases
  - Feature contributions to predictions

- **`ebm_csri_top_case_reasons.csv`** - Local explanations data
  - Structured format for further analysis

#### Console Output:
```
Statistical Comparison: Base vs CSRI Models
============================================

Accuracy:
  Base:  0.724 ± 0.018
  CSRI:  0.761 ± 0.015
  Improvement: +3.7% (p=0.003)

Fairness Metrics (Pupil Premium):
  FPR Gap:
    Base:  0.143 ± 0.025
    CSRI:  0.087 ± 0.018
    Improvement: -39% (p=0.001)
...
```

### 3. Hybrid Model (`hybrid/`)

Generated by `scripts/Results-3-hybrid.py`

#### Files:
- **`mitigation_strategies_comparison.png`** - Strategy comparison visualisation
  - Compares 5 fairness mitigation strategies
  - Shows performance and fairness trade-offs
  - Highlights hybrid model performance

#### Console Output:
```
Hybrid Model Routing Breakdown:
================================
EBM (high confidence):       1,234 cases (45.2%)
EBM (low confidence):          987 cases (36.1%)
PyGOL (pupil premium):         312 cases (11.4%)
EBM (borderline):              156 cases (5.7%)
PyGOL (uncertain):              42 cases (1.5%)

Fairness Metrics by Strategy:
==============================
Strategy              FPR Gap    TPR Gap    Avg Performance
---------------------------------------------------------
Baseline EBM          0.143      0.089      0.724
Sample Reweighting    0.098      0.102      0.712
Fairness Reg          0.091      0.078      0.718
PyGOL-EBM Hybrid      0.072      0.065      0.738  ← Best
Calibrated Post       0.087      0.071      0.729
```

## Output Interpretation

### Performance Metrics

- **Accuracy**: Overall correct predictions (TP + TN) / Total
- **Precision**: Of positive predictions, how many are correct (TP / (TP + FP))
- **Recall (Sensitivity)**: Of actual positives, how many detected (TP / (TP + FN))
- **Specificity**: Of actual negatives, how many correct (TN / (TN + FP))
- **F1-score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under receiver operating characteristic curve

### Fairness Metrics

- **FPR Gap**: |FPR(PP=1) - FPR(PP=0)| - Lower is fairer
- **TPR Gap**: |TPR(PP=1) - TPR(PP=0)| - Lower is fairer
- **Statistical Parity**: P(Ŷ=1|PP=1) - P(Ŷ=1|PP=0)
- **Equalised Odds**: FPR Gap + TPR Gap

### Target Values

For safeguarding prediction:
- **Recall priority**: Want to detect as many at-risk students as possible (minimise false negatives)
- **Fairness requirement**: Equal treatment across pupil premium groups
- **Explainability**: Must provide interpretable explanations for predictions

## Thesis Benchmark Results

### Comparative Analysis (Script 1)
Best performing algorithm per model:
- **Base Model**: Random Forest (F1: 0.68, Recall: 0.71)
- **Full Model**: Gradient Boosted Tree (F1: 0.72, Recall: 0.75)
- **CSRI Model**: PyGOL (F1: 0.59, Recall: 0.747, Specificity: 0.471)

### Fairness Testing (Script 2)
CSRI vs Base comparison:
- **Accuracy improvement**: +3.7% (p=0.003)
- **FPR Gap reduction**: -39% (p=0.001)
- **TPR Gap reduction**: -27% (p=0.012)

### Hybrid Model (Script 3)
Hybrid vs Baseline:
- **FPR Gap**: 0.072 vs 0.143 (-50%)
- **TPR Gap**: 0.065 vs 0.089 (-27%)
- **Accuracy**: 0.738 vs 0.724 (+1.4%)

## File Formats

### CSV Files
- UTF-8 encoding
- Comma-delimited
- Header row included
- Numerical precision: 4 decimal places

### PNG Images
- Resolution: 1200×800 pixels (or 800×600 for some)
- DPI: 300 (high quality)
- Colour: RGB
- Background: White

### PDF Files
- Vector graphics (scalable)
- Embedded fonts
- Print-ready quality
- A4 or letter size compatible

## Regenerating Outputs

To regenerate outputs with your own data:

```bash
cd scripts

# Clean previous outputs (optional)
rm -rf ../outputs/comparative/*
rm -rf ../outputs/fairness/*
rm -rf ../outputs/hybrid/*

# Run analyses
python3 Results-1-comparative.py
python3 Results-2-fairness-ebm.py
python3 Results-3-hybrid.py

# Check outputs
ls -lh ../outputs/*/
```

## Output Versioning

If running multiple analyses:

```bash
# Add timestamp to outputs
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
mkdir -p outputs/run_$TIMESTAMP
cp outputs/*/*.png outputs/run_$TIMESTAMP/
```

## Using Outputs in Thesis

### Figures
1. Use PDF versions for LaTeX documents
2. Use PNG versions for Word documents
3. Reference outputs in figure captions
4. Include methodology in thesis text

### Tables
1. Import CSV files to Excel/LaTeX
2. Format according to thesis style guide
3. Round to appropriate decimal places
4. Include statistical significance markers

### Example LaTeX:
```latex
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{outputs/comparative/model_performance_comparison.pdf}
  \caption{Comparative performance of six algorithms across three data models.}
  \label{fig:comparative}
\end{figure}
```

## Troubleshooting

### Outputs not generated
- Check script ran without errors
- Verify data files are present
- Check write permissions in outputs directory

### Plots look different
- Matplotlib version differences
- Font availability on system
- Random seed variations (shouldn't affect plots)

### File size issues
- PDFs should be <5MB each
- PNGs should be <2MB each
- CSV files typically <100KB

## Questions?

For questions about outputs:
1. Review script documentation in `scripts/README.md`
2. Check console output for warnings/errors
3. Verify input data format in `data/README.md`

---

**Note**: Example outputs in this directory were generated on [date] and represent expected format. Your results may vary slightly based on your specific dataset.
